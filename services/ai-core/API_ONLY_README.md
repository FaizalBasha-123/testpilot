# AI Core Service - API-Only Configuration

## ‚ö° Zero Local ML Models - 100% Cloud APIs

This service has been **stripped of all PyTorch, TensorFlow, and local ML model dependencies**. It now uses Groq's LLM API directly (OpenAI-compatible endpoint).

### ‚ùå What Was Removed (Local ML Models)

- ‚ùå **sentence-transformers** (2GB+ PyTorch models - for local embeddings)
- ‚ùå **lancedb** (requires local embedding models)
- ‚ùå **torch** (PyTorch framework)
- ‚ùå **transformers** (HuggingFace local models)
- ‚ùå **langchain** (heavy dependency chain)
- ‚ùå All local embedding model files

### ‚úÖ What Remains

**LLM API Clients (Cloud-based):**
- ‚úÖ **Groq** - Direct LLM API (OpenAI-compatible endpoint)
- ‚úÖ **OpenAI** - Embeddings + optional LLMs
- ‚úÖ **Anthropic** - Optional LLMs
- ‚úÖ **Google Generative AI** - Optional LLMs
- ‚úÖ **Azure OpenAI** - Optional LLMs
- ‚úÖ **AWS Bedrock** - Optional LLMs

**Infrastructure Components (NOT ML):**
- ‚úÖ **Qdrant** - Vector database for code context (uses cloud embeddings)
- ‚úÖ **Redis** - Caching, job queues, rate limiting
- ‚úÖ **PostgreSQL** - User data, credits, metadata

> **Important:** Qdrant and Redis are **infrastructure**, not ML models. Qdrant stores vectors generated by **cloud embedding APIs** (OpenAI, Cohere, Voyage), not local models.

### üöÄ Benefits

| Metric | Before (with PyTorch) | After (API-only) |
|--------|----------------------|------------------|
| **Docker Image Size** | ~3.5 GB | ~500 MB |
| **Build Time** | 10-15 minutes | 2-3 minutes |
| **Memory Usage** | ~2 GB | ~200 MB |
| **Dependencies** | 150+ packages | 40 packages |
| **GPU Required** | Yes (for embeddings) | No |

## Configuration

### Required Environment Variables

```bash
# Groq Configuration (Direct API)
GROQ_API_KEY=your_groq_api_key_here
CONFIG__MODEL=groq/llama-3.3-70b-versatile

# Optional Groq override
GROQ_API_BASE=https://api.groq.com/openai/v1

# Optional embeddings (OpenAI-compatible)
OPENAI_API_KEY=your_openai_key_here
EMBEDDING_MODEL=text-embedding-3-small
```

### Supported Groq Models (Direct API)

1. **Groq** (Recommended - Fast & Free Tier)
   - `groq/llama-3.3-70b-versatile`
   - `groq/mixtral-8x7b-32768`

If you need non-Groq providers, add a dedicated handler for that API.

## üèóÔ∏è Architecture: Code Context & Embeddings

### How Context Works (Without Local Models)

The AI-Core uses a **hybrid approach** for code context:

1. **AST Parsing (No ML)**: Tree-sitter for syntax analysis
2. **Cloud Embeddings**: OpenAI/Cohere/Voyage APIs for semantic search
3. **Qdrant Storage**: Stores vectors generated by cloud APIs
4. **Redis Caching**: Caches API responses to reduce costs

```
Code File ‚Üí Tree-sitter Parse ‚Üí Context Chunks
                                      ‚Üì
                            OpenAI Embeddings API
                                      ‚Üì
                            Qdrant Vector Store
                                      ‚Üì
                            Similarity Search ‚Üí LLM Context
```

### Qdrant + Cloud Embeddings

**Qdrant is NOT removed** because:
- ‚úÖ It's a database, not an ML model
- ‚úÖ Stores vectors from **cloud embedding APIs**
- ‚úÖ Essential for code similarity/context search
- ‚úÖ No local compute needed (just stores floats)

**Example Configuration:**
```bash
# Use OpenAI embeddings instead of sentence-transformers
EMBEDDING_API_BASE=https://api.openai.com/v1
EMBEDDING_MODEL=text-embedding-3-small
OPENAI_API_KEY=your_key_here

# Qdrant connection (local Docker or cloud)
QDRANT_URL=http://qdrant:6333
```

### Redis for Performance

**Redis is NOT removed** because:
- ‚úÖ Infrastructure component (caching, queues)
- ‚úÖ NOT related to ML/embeddings
- ‚úÖ Reduces API costs by caching responses
- ‚úÖ Handles background jobs (async analysis)

**What Redis Caches:**
- LLM API responses (Groq, OpenAI)
- Code analysis results
- Embedding API responses
- Rate limit counters

## What Was Actually Removed

### ‚ùå Only Local ML Dependencies

- `sentence-transformers` ‚Üí Use **OpenAI Embeddings API**
- `torch` (PyTorch) ‚Üí Use **Cloud LLMs**
- `lancedb` ‚Üí Already using **Qdrant** (no change needed)
- Local model files ‚Üí Use **API keys**

## Build and Deploy

```bash
# Build the service
docker-compose build ai-core

# Start the service
docker-compose up -d ai-core

# Check logs (should see NO PyTorch/tensor mentions)
docker-compose logs -f ai-core
```

### Expected Build Output (Clean)

‚úÖ **You should see:**
- Installing fastapi
- Installing openai
- Installing anthropic
- Installing litellm

‚ùå **You should NOT see:**
- Installing torch
- Installing sentence-transformers
- Building wheel for scipy
- Downloading PyTorch binaries
- Installing nvidia-cuda

## Verification

```bash
# Check installed packages (should be small)
docker exec blackbox-ai-core pip list | grep -E "torch|tensor|sentence"

# Expected: NOTHING or only harmless utilities
```

## Troubleshooting

### Still seeing PyTorch during build?

1. **Clear Docker cache:**
   ```bash
   docker-compose build --no-cache ai-core
   ```

2. **Check requirements.txt:**
   ```bash
   cat services/ai-core/requirements.txt | grep -E "sentence|torch|lance"
   ```
   Should return NOTHING.

3. **Check transitive dependencies:**
   Some packages might pull in PyTorch indirectly. If you see this:
   ```bash
   docker run --rm blackbox-ai-core pip show sentence-transformers
   ```
   Should return: `WARNING: Package(s) not found`

### Need embeddings?

Use cloud-based embedding APIs:
- **OpenAI Embeddings API** (`text-embedding-3-small`)
- **Cohere Embeddings API**
- **Voyage AI Embeddings**

All supported via OpenAI-compatible embedding endpoints (Groq or OpenAI).

## Performance

### API Latency Comparison

| Task | Local Model | Cloud API (Groq) |
|------|------------|------------------|
| Code review | ~5-10s | ~2-3s |
| Issue analysis | ~3-5s | ~1-2s |
| Startup time | ~30s | <1s |

Cloud APIs are **faster** because:
- No model loading time
- Optimized infrastructure
- Lower memory overhead

## Cost Considerations

### Groq (Recommended for Development)
- **Free tier**: 30 requests/minute
- **Cost**: Free for most use cases
- **Speed**: Extremely fast (llama-3.3-70b)

### OpenAI
- **gpt-3.5-turbo**: $0.50-2.00 per 1M tokens
- **gpt-4**: $10-60 per 1M tokens

### Best Practice
Use Groq's free tier for development, OpenAI for production if needed.

---

**Status**: ‚úÖ API-Only (No Local ML Models)  
**Last Updated**: February 11, 2026  
**Image Size**: ~500 MB (vs 3.5 GB before)
